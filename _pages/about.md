---
layout: about
title: about
permalink: /
description: <a href="#">Affiliations</a>. Address. Contacts. Moto. Etc.

profile:
  align: right
  image: piyush-3.png
  address: >

news: true
social: true
---

I am pursuing Master's in AI at the [University of Amsterdam](https://www.uva.nl/en). Along with the coursework, I conduct research on video understanding at the [VISLab](https://ivi.fnwi.uva.nl/vislab/) advised by Prof. dr. [Cees Snoek](https://www.ceessnoek.info/) and Dr. [Makarand Tapaswi](https://www.iiit.ac.in/people/faculty/Makarand/)
<!-- (IIIT Hyderabad, India) -->.

<!-- Hi! I will be starting my Master's in AI at [University of Amsterdam](https://www.uva.nl/en) in Sept 2021. 
I am passionate about research and have developed interest in a variety of areas: video understanding, 3D vision, probabilistic modelling & inference. I also have newly-found curiosity in geometric deep learning, deep generative models and so many other exciting areas in machine learning that evolve each day. -->

In the past, I've been a Research Fellow at [Wadhwani AI](http://wadhwaniai.org){:target="\_blank"} where I primarily worked on [estimating infant anthropometry from a monocular video](https://www.wadhwaniai.org/work/maternal-newborn-child-health/). During the pandemic, I also worked on [using cough as a biomarker to detect presence of CoVID-19](https://www.wadhwaniai.org/programs/cough-against-covid/).

Before that, I completed my Bachelor's in Mathematics at [IIT Kanpur](https://iitk.ac.in) where I worked with 
Dr. [Piyush Rai](https://www.cse.iitk.ac.in/users/piyush/) and Dr. [Swaprava Nath](https://www.cse.iitb.ac.in/~swaprava/).

<!-- Before that, I completed my Bachelor's in Mathematics at [IIT Kanpur](https://iitk.ac.in) where I enjoyed my coursework, especially courses on probabilistic modelling taught by [Dr. Piyush Rai](https://www.cse.iitk.ac.in/users/piyush/). During this time, I worked on a variety of mini-projects on a variety of topics: Bayesian optimization, zero-shot learning and understanding rotations in 3D. -->

<!-- During my undergrad, I was fortunate to have been a Research Intern at [Adobe Research](https://research.adobe.com/) where I worked on privacy and bias in ML models.  -->


<style>
  .profilelinks {
    display: flex;
    float: center;
    flex-direction: row;
    justify-content: center;
    align-items: center;
    margin-bottom: 1em;
  }
  .singlelink {
    color: #243b55;
    font-size: 1.4em;
  }
  .singlelink:hover {
    color: #0076df;
  }
</style>

<div class="profilelinks">
<!-- <a href="./assets/pdf/piyush-cv-v1.0.pdf" style="font-size: 23px">CV</a> -->
<a class="singlelink" href="./assets/pdf/piyush-cv-v1.0.pdf" target="_blank" title="CV"><i class="far fa-id-badge"></i></a>
&nbsp; &nbsp; 
<a class="singlelink" href="https://scholar.google.com/citations?user={{ site.scholar_userid }}" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>
&nbsp; &nbsp; 
<a class="singlelink" href="https://github.com/{{ site.github_username }}" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
&nbsp; &nbsp; 
<a class="singlelink" href="https://twitter.com/{{ site.twitter_username }}" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a>
&nbsp; &nbsp;
<a class="singlelink" href="https://www.linkedin.com/in/{{ site.linkedin_username }}" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a>
</div>


<br>

<!-- <br> -->

<!-- <img src="https://raw.githubusercontent.com/fmthoker/SEVERE-BENCHMARK/main/media/concept_figure.png" alt="drawing" style="width:300px;"/>
This is sample text wrapped around image. Let me check how long can it go? -->

<style>
.mydiv {
    /* width: 300px; */
    /* background-color: #ffcc33; */
    width: 155%;
    float: left;
    /* background-color: green; */
    /* margin: auto; */
    padding-top: 2%;
    padding-bottom: 2%;
    margin-bottom: 2%;
}
.leftdiv {
    /* width: 300px; */
    /* background-color: #ffcc33; */
    width: 40%;
    float: left;
    /* background-color: red; */
    margin: auto;
    transition: transform .2s; /* Animation */
    padding-right: 1%;
    justify-content: center;
    align-items: center;
    vertical-align: middle;
}
.rightdiv {
    /* width: 300px; */
    /* background-color: #ffcc33; */
    width: 60%;
    float: right;
    /* background-color: blue; */
    margin: auto;
    padding-left: 1%;
    vertical-align: middle;
}

.leftdiv:hover {
  transform: scale(2.5); /* (250% zoom - Note: if the zoom is too large, it will go outside of the viewport) */
}

.papertitle {
    /* font-size: 1.1vw; */
    font-size: 0.95em;
    font-weight: bold;
    margin-bottom: 1%;
}
.paperauthors {
    /* font-size: 0.9vw; */
    font-size: 0.8em;
    margin-bottom: 0.;
}
.papervenue {
    /* font-size: 0.9vw; */
    font-size: 0.8em;
    margin-bottom: 0.;
    font-style: italic;
}

.paperfootnote {
    /* font-size: 0.9vw; */
    font-size: 0.7em;
    padding-bottom: 1%;
    margin-top: 0.;
}

.paperlinks {
    /* font-size: 0.9vw; */
    font-size: 0.7em;
    margin-top: 1%;
    font-weight: 400;
}

hr {
    /* set width same as mydiv */
    width: 100%;
    height: 2px;
    /* Set the hr color */
    color: #f2f2f2;
    padding: 0;
    margin-top: 0;
    margin-bottom: 0;
}

</style>


### Selected publications

<div class="mydiv">
  <h5> <b>2022</b></h5>
  <hr class="hr">
</div>

<!-- draw a gray colored horizontal line -->
<!-- <div class="mydiv" style="background-color: #f2f2f2;"> -->


<div class="mydiv">
    <div class="leftdiv">
      <img src="./assets/img/severe-teaser-2.gif" alt="drawing" style="width:100%;"/>
    </div>
    <div class="rightdiv">
      <div class="papertitle"><a href="https://arxiv.org/abs/2203.14221">How Severe is Benchmark-Sensitivity in Video Self-Supervised Learning?</a></div>
      <div class="paperauthors">Fida Mohammad Thoker, Hazel Doughty, <b><u>Piyush Bagad</u></b>, Cees Snoek</div>
      <div class="papervenue">ECCV, 2022 (Tel Aviv, Israel)</div>
      <div class="paperlinks"> <a href="https://arxiv.org/abs/2203.14221">ArXiv</a> &nbsp; | &nbsp; <a href="https://bpiyush.github.io/SEVERE-website/">Project page</a> &nbsp; | &nbsp; <a href="https://github.com/fmthoker/SEVERE-BENCHMARK">Code</a> </div>
    </div>
<!-- <hr style="width:100%;text-align:left;margin-left:0;margin-right:0;margin-bottom:0;"> -->
</div>
<!-- <p><small>Work done during Master AI at UvA.</small></p> -->
<!-- <div class="paperfootnote">Work done during Master AI at UvA.</div> -->

<div class="mydiv">
    <div class="leftdiv">
      <img src="./assets/img/c3po-1.gif" alt="drawing" style="width:100%;"/>
    </div>
    <div class="rightdiv">
      <div class="papertitle"><a href="assets/pdf/c3po-camera-ready_compressed.pdf">C-3PO: Towards Rotation Equivariant Feature Detection and Description</a></div>
      <div class="paperauthors"><b><u>Piyush Bagad</u></b>*,  Floor Eijkelboom*, Mark Fokkema*, Danilo de Goede*, Paul Hilders*, Miltiadis Kofinas</div>
      <div class="papervenue"><a href="https://vipriors.github.io/"> VIPrior Workshop</a>, ECCV, 2022 (Tel Aviv, Israel)</div>
      <div class="paperlinks"> <a href="assets/pdf/c3po-camera-ready_compressed.pdf">Paper</a> &nbsp; | &nbsp; Project page &nbsp; |  &nbsp; <a href="https://github.com/bpiyush/rotation-equivariant-lfm" download>Code</a> &nbsp; | &nbsp; <a href="assets/pdf/c3po_poster.pdf" download>Poster</a> &nbsp; | &nbsp; <a href="assets/pdf/c3po.pptx" download>Oral Presentation</a> </div>
    </div>
</div>

<div class="mydiv">
  <h5> <b>2021</b></h5>
  <hr class="hr">
</div>

<div class="mydiv">
    <div class="leftdiv">
      <img src="./assets/img/adobe-v2.gif" alt="drawing" style="width:100%;"/>
    </div>
    <div class="rightdiv">
      <div class="papertitle"><a href="https://dl.acm.org/doi/pdf/10.1145/3437963.3441712?casa_token=B3iCVyG_w8oAAAAA:MQtAcsFR6Zpupi_yzWHCTknzCCRvIi4Z9uMAvwTeZx3ZcctSOQUAUIVNNNIDGkvy7DAPqVY0nVnerg">Data-Sharing Economy: Value-Addition from Data meets Privacy</a></div>
      <div class="paperauthors"><b><u>Piyush Bagad</u></b>,  Subrata Mitra, Sunny Dhamnani, Atanu R Sinha, Raunak Gautam, Haresh Khanna</div>
      <div class="papervenue"><a href="https://www.wsdm-conference.org/2021/accepted-demos.php"> WSDM Demos</a>, 2021 (Virtual event, Israel)</div>
      <div class="papervenue">Work done during Internship at <a href="https://research.adobe.com/careers/bangalore/">Adobe Research (India)</a> in 2018.</div>
      <div class="paperlinks"> <a href="https://dl.acm.org/doi/pdf/10.1145/3437963.3441712?casa_token=B3iCVyG_w8oAAAAA:MQtAcsFR6Zpupi_yzWHCTknzCCRvIi4Z9uMAvwTeZx3ZcctSOQUAUIVNNNIDGkvy7DAPqVY0nVnerg">Paper</a> &nbsp; | &nbsp; <a href="https://drive.google.com/file/d/1WkX1sr0gkex0acYLIKP93zrCnwhaoVwb/view?usp=sharing">Video</a> &nbsp; | &nbsp; <a href="https://github.com/bpiyush/privacy-GAN">Code</a> </div>
    </div>
</div>


<h3>Selected projects</h3>

<div class="mydiv">
    <div class="leftdiv">
      <img src="./assets/img/re-cgn-cluster-1.png" alt="drawing" style="width:100%;"/>
    </div>
    <div class="rightdiv">
      <div class="papertitle"><a href="assets/pdf/re-cgn.pdf">Reproducibility Study of "Counterfactual Generative Networks"</a></div>
      <div class="paperauthors"><b><u>Piyush Bagad</u></b>*,  Paul Hilders*,  Jesse Maas*,  Danilo de Goede*</div>
      <div class="papervenue" style="color:red;"><a href="https://paperswithcode.com/rc2021" style="color:red;">Best paper award</a></div>
      <div class="papervenue"><a href="https://paperswithcode.com/rc2021"> ML Reproducibility Challenge</a>, ReScience Journal</div>
      <div class="papervenue">Invited to present at <a href="https://blog.neurips.cc/2022/08/15/journal-showcase/" style="color:red;">Journal Showcase Poster Session @ NeurIPS 2022.</a></div>
      <div class="paperlinks"> <a href="https://zenodo.org/record/6574635#.YviVUexBy3J">Paper</a> &nbsp; | &nbsp; <a href="https://github.com/danilodegoede/Re-CGN">Code</a> </div>
    </div>
</div>

<div class="mydiv">
    <div class="leftdiv">
      <img src="./assets/img/clip-grounding-v5.gif" alt="drawing" style="width:100%;"/>
    </div>
    <div class="rightdiv">
      <div class="papertitle"><a href="assets/pdf/re-cgn.pdf">Quantifying CLIP’s ability to Perform Cross-Modal
Grounding Using Attention-Model Explainability</a></div>
      <div class="paperauthors"><b><u>Piyush Bagad</u></b>*,  Paul Hilders*, Danilo de Goede*</div>
      <div class="papervenue" style="color:red;">Best poster award in the I&E course at UvA</div>
      <div class="paperlinks"> <a href="./assets/pdf/clip-grounding-poster.pdf">Poster</a> &nbsp; | &nbsp; <a href="https://github.com/bpiyush/CLIP-grounding">Code</a> &nbsp; | &nbsp; <a href="https://huggingface.co/spaces/PaulHilders/CLIPGroundingExplainability">Demo</a></div>
    </div>
</div>


<h3>AI in the real-world</h3>

<div class="mydiv">
    <div class="leftdiv">
      <img src="./assets/img/cac-4-looped.gif" alt="drawing" style="width:100%;"/>
    </div>
    <div class="rightdiv">
      <div class="papertitle"><a href="https://arxiv.org/abs/2009.08790">Cough Against COVID: Evidence of COVID-19 Signature in Cough Sounds</a></div>
      <div class="paperauthors"><b><u>Piyush Bagad</u></b>, Aman Dalmia, Jigar Doshi, Arsha Nagrani, Parag Bhamare, Amrita Mahale, Saurabh Rane, Neeraj Agarwal, Rahul Panicker</div>
      <div class="papervenue">ArXiv, 2021 (Mumbai, India)</div>
      <div class="paperlinks"> <a href="https://arxiv.org/abs/2009.08790">ArXiv</a> &nbsp; | &nbsp; <a href="https://www.wadhwaniai.org/programs/cough-against-covid/">Project page</a> &nbsp; | &nbsp; <a href="https://github.com/WadhwaniAI/cough-against-covid">Code</a> </div>
      <div class="paperfootnote">Media credits: <a href="https://www.wadhwaniai.org/programs/cough-against-covid/">Wadhwani AI</a></div>
    </div>
</div>

<div class="mydiv">
    <div class="leftdiv">
      <img src="./assets/img/anthro-3-looped.gif" alt="drawing" style="width:100%;"/>
    </div>
    <div class="rightdiv">
      <div class="papertitle"><a href="https://www.wadhwaniai.org/programs/newborn-anthropometry/">Newborn Anthropometry: Estimating infant weight from a monocular video</a></div>
      <div class="paperauthors"><b><u>Piyush Bagad</u></b>, Aman Dalmia, Makarand Tapaswi, Sansiddh Jain, Aditya A Sarma, Namrata Deka, Jigar Doshi, Parag Bhamare, Amrita Mahale, Rahul Panicker (Not in order)</div>
      <div class="papervenue">Unpublished, 2019-2021 (Mumbai, India)</div>
      <div class="paperlinks"> ArXiv &nbsp; | &nbsp; <a href="https://www.wadhwaniai.org/programs/newborn-anthropometry/">Project page</a> &nbsp; | &nbsp; Code </div>
      <div class="paperfootnote">Media credits: <a href="https://www.wadhwaniai.org/programs/newborn-anthropometry/">Wadhwani AI</a></div>
    </div>
</div>
